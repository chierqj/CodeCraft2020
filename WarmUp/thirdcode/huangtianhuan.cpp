#include <arm_neon.h>
#include <fcntl.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/mman.h>
#include <sys/types.h>
#include <unistd.h>
#include <cmath>
#include <cstdlib>
#include <fstream>
#include <iostream>
#include <sstream>
#include <thread>
#include <vector>
using namespace std;

class LR {
 public:
  void train();
  void predict();
  int loadModel();
  int storeModel();
  LR(string trainFile, string testFile, string predictOutFile);

 private:
  int *predictVec;
  float *param;
  string trainFile;
  string testFile;
  string predictOutFile;
  int test_num = 0;
  float *subtrain0 = (float *)malloc(640 * 1001 * sizeof(float));
  float *subtrain1 = (float *)malloc(640 * 1001 * sizeof(float));
  float *subtrain2 = (float *)malloc(640 * 1001 * sizeof(float));
  float *subtrain3 = (float *)malloc(640 * 1001 * sizeof(float));
  float *subtrain4 = (float *)malloc(640 * 1001 * sizeof(float));
  float *subtrain5 = (float *)malloc(640 * 1001 * sizeof(float));
  float *subtrain6 = (float *)malloc(640 * 1001 * sizeof(float));
  float *subtrain7 = (float *)malloc(800 * 1001 * sizeof(float));

  int sub_num_0 = 0;
  int sub_num_1 = 0;
  int sub_num_2 = 0;
  int sub_num_3 = 0;
  int sub_num_4 = 0;
  int sub_num_5 = 0;
  int sub_num_6 = 0;
  int sub_num_7 = 0;

 private:
  bool init();
  bool loadTrainData();
  int storePredict(int *predictVec);
  void initParam();
  float wxbCalcTrain(float *subtrain, const int index);
  float lossCal();

 private:
  const int featuresNum = 1000;
  const float wtInitV = 0.25;
  const float rate = 0.00125;
  const float predictTrueThresh = 0.5;
  const int train_num = 5000;

  const int batch_size = 8;
  const int epoch = 1;
};

LR::LR(string trainF, string testF, string predictOutF) {
  trainFile = trainF;
  testFile = testF;
  predictOutFile = predictOutF;
  init();
}

float array_result[3001] = {
    0.000, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.010,
    0.011, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017, 0.018, 0.019, 0.020, 0.021,
    0.022, 0.023, 0.024, 0.025, 0.026, 0.027, 0.028, 0.029, 0.030, 0.031, 0.032,
    0.033, 0.034, 0.035, 0.036, 0.037, 0.038, 0.039, 0.040, 0.041, 0.042, 0.043,
    0.044, 0.045, 0.046, 0.047, 0.048, 0.049, 0.050, 0.051, 0.052, 0.053, 0.054,
    0.055, 0.056, 0.057, 0.058, 0.059, 0.060, 0.061, 0.062, 0.063, 0.064, 0.065,
    0.066, 0.067, 0.068, 0.069, 0.070, 0.071, 0.072, 0.073, 0.074, 0.075, 0.076,
    0.077, 0.078, 0.079, 0.080, 0.081, 0.082, 0.083, 0.084, 0.085, 0.086, 0.087,
    0.088, 0.089, 0.090, 0.091, 0.092, 0.093, 0.094, 0.095, 0.096, 0.097, 0.098,
    0.099, 0.100, 0.101, 0.102, 0.103, 0.104, 0.105, 0.106, 0.107, 0.108, 0.109,
    0.110, 0.111, 0.112, 0.113, 0.114, 0.115, 0.116, 0.117, 0.118, 0.119, 0.120,
    0.121, 0.122, 0.123, 0.124, 0.125, 0.126, 0.127, 0.128, 0.129, 0.130, 0.131,
    0.132, 0.133, 0.134, 0.135, 0.136, 0.137, 0.138, 0.139, 0.140, 0.141, 0.142,
    0.143, 0.144, 0.145, 0.146, 0.147, 0.148, 0.149, 0.150, 0.151, 0.152, 0.153,
    0.154, 0.155, 0.156, 0.157, 0.158, 0.159, 0.160, 0.161, 0.162, 0.163, 0.164,
    0.165, 0.166, 0.167, 0.168, 0.169, 0.170, 0.171, 0.172, 0.173, 0.174, 0.175,
    0.176, 0.177, 0.178, 0.179, 0.180, 0.181, 0.182, 0.183, 0.184, 0.185, 0.186,
    0.187, 0.188, 0.189, 0.190, 0.191, 0.192, 0.193, 0.194, 0.195, 0.196, 0.197,
    0.198, 0.199, 0.200, 0.201, 0.202, 0.203, 0.204, 0.205, 0.206, 0.207, 0.208,
    0.209, 0.210, 0.211, 0.212, 0.213, 0.214, 0.215, 0.216, 0.217, 0.218, 0.219,
    0.220, 0.221, 0.222, 0.223, 0.224, 0.225, 0.226, 0.227, 0.228, 0.229, 0.230,
    0.231, 0.232, 0.233, 0.234, 0.235, 0.236, 0.237, 0.238, 0.239, 0.240, 0.241,
    0.242, 0.243, 0.244, 0.245, 0.246, 0.247, 0.248, 0.249, 0.250, 0.251, 0.252,
    0.253, 0.254, 0.255, 0.256, 0.257, 0.258, 0.259, 0.260, 0.261, 0.262, 0.263,
    0.264, 0.265, 0.266, 0.267, 0.268, 0.269, 0.270, 0.271, 0.272, 0.273, 0.274,
    0.275, 0.276, 0.277, 0.278, 0.279, 0.280, 0.281, 0.282, 0.283, 0.284, 0.285,
    0.286, 0.287, 0.288, 0.289, 0.290, 0.291, 0.292, 0.293, 0.294, 0.295, 0.296,
    0.297, 0.298, 0.299, 0.300, 0.301, 0.302, 0.303, 0.304, 0.305, 0.306, 0.307,
    0.308, 0.309, 0.310, 0.311, 0.312, 0.313, 0.314, 0.315, 0.316, 0.317, 0.318,
    0.319, 0.320, 0.321, 0.322, 0.323, 0.324, 0.325, 0.326, 0.327, 0.328, 0.329,
    0.330, 0.331, 0.332, 0.333, 0.334, 0.335, 0.336, 0.337, 0.338, 0.339, 0.340,
    0.341, 0.342, 0.343, 0.344, 0.345, 0.346, 0.347, 0.348, 0.349, 0.350, 0.351,
    0.352, 0.353, 0.354, 0.355, 0.356, 0.357, 0.358, 0.359, 0.360, 0.361, 0.362,
    0.363, 0.364, 0.365, 0.366, 0.367, 0.368, 0.369, 0.370, 0.371, 0.372, 0.373,
    0.374, 0.375, 0.376, 0.377, 0.378, 0.379, 0.380, 0.381, 0.382, 0.383, 0.384,
    0.385, 0.386, 0.387, 0.388, 0.389, 0.390, 0.391, 0.392, 0.393, 0.394, 0.395,
    0.396, 0.397, 0.398, 0.399, 0.400, 0.401, 0.402, 0.403, 0.404, 0.405, 0.406,
    0.407, 0.408, 0.409, 0.410, 0.411, 0.412, 0.413, 0.414, 0.415, 0.416, 0.417,
    0.418, 0.419, 0.420, 0.421, 0.422, 0.423, 0.424, 0.425, 0.426, 0.427, 0.428,
    0.429, 0.430, 0.431, 0.432, 0.433, 0.434, 0.435, 0.436, 0.437, 0.438, 0.439,
    0.440, 0.441, 0.442, 0.443, 0.444, 0.445, 0.446, 0.447, 0.448, 0.449, 0.450,
    0.451, 0.452, 0.453, 0.454, 0.455, 0.456, 0.457, 0.458, 0.459, 0.460, 0.461,
    0.462, 0.463, 0.464, 0.465, 0.466, 0.467, 0.468, 0.469, 0.470, 0.471, 0.472,
    0.473, 0.474, 0.475, 0.476, 0.477, 0.478, 0.479, 0.480, 0.481, 0.482, 0.483,
    0.484, 0.485, 0.486, 0.487, 0.488, 0.489, 0.490, 0.491, 0.492, 0.493, 0.494,
    0.495, 0.496, 0.497, 0.498, 0.499, 0.500, 0.501, 0.502, 0.503, 0.504, 0.505,
    0.506, 0.507, 0.508, 0.509, 0.510, 0.511, 0.512, 0.513, 0.514, 0.515, 0.516,
    0.517, 0.518, 0.519, 0.520, 0.521, 0.522, 0.523, 0.524, 0.525, 0.526, 0.527,
    0.528, 0.529, 0.530, 0.531, 0.532, 0.533, 0.534, 0.535, 0.536, 0.537, 0.538,
    0.539, 0.540, 0.541, 0.542, 0.543, 0.544, 0.545, 0.546, 0.547, 0.548, 0.549,
    0.550, 0.551, 0.552, 0.553, 0.554, 0.555, 0.556, 0.557, 0.558, 0.559, 0.560,
    0.561, 0.562, 0.563, 0.564, 0.565, 0.566, 0.567, 0.568, 0.569, 0.570, 0.571,
    0.572, 0.573, 0.574, 0.575, 0.576, 0.577, 0.578, 0.579, 0.580, 0.581, 0.582,
    0.583, 0.584, 0.585, 0.586, 0.587, 0.588, 0.589, 0.590, 0.591, 0.592, 0.593,
    0.594, 0.595, 0.596, 0.597, 0.598, 0.599, 0.600, 0.601, 0.602, 0.603, 0.604,
    0.605, 0.606, 0.607, 0.608, 0.609, 0.610, 0.611, 0.612, 0.613, 0.614, 0.615,
    0.616, 0.617, 0.618, 0.619, 0.620, 0.621, 0.622, 0.623, 0.624, 0.625, 0.626,
    0.627, 0.628, 0.629, 0.630, 0.631, 0.632, 0.633, 0.634, 0.635, 0.636, 0.637,
    0.638, 0.639, 0.640, 0.641, 0.642, 0.643, 0.644, 0.645, 0.646, 0.647, 0.648,
    0.649, 0.650, 0.651, 0.652, 0.653, 0.654, 0.655, 0.656, 0.657, 0.658, 0.659,
    0.660, 0.661, 0.662, 0.663, 0.664, 0.665, 0.666, 0.667, 0.668, 0.669, 0.670,
    0.671, 0.672, 0.673, 0.674, 0.675, 0.676, 0.677, 0.678, 0.679, 0.680, 0.681,
    0.682, 0.683, 0.684, 0.685, 0.686, 0.687, 0.688, 0.689, 0.690, 0.691, 0.692,
    0.693, 0.694, 0.695, 0.696, 0.697, 0.698, 0.699, 0.700, 0.701, 0.702, 0.703,
    0.704, 0.705, 0.706, 0.707, 0.708, 0.709, 0.710, 0.711, 0.712, 0.713, 0.714,
    0.715, 0.716, 0.717, 0.718, 0.719, 0.720, 0.721, 0.722, 0.723, 0.724, 0.725,
    0.726, 0.727, 0.728, 0.729, 0.730, 0.731, 0.732, 0.733, 0.734, 0.735, 0.736,
    0.737, 0.738, 0.739, 0.740, 0.741, 0.742, 0.743, 0.744, 0.745, 0.746, 0.747,
    0.748, 0.749, 0.750, 0.751, 0.752, 0.753, 0.754, 0.755, 0.756, 0.757, 0.758,
    0.759, 0.760, 0.761, 0.762, 0.763, 0.764, 0.765, 0.766, 0.767, 0.768, 0.769,
    0.770, 0.771, 0.772, 0.773, 0.774, 0.775, 0.776, 0.777, 0.778, 0.779, 0.780,
    0.781, 0.782, 0.783, 0.784, 0.785, 0.786, 0.787, 0.788, 0.789, 0.790, 0.791,
    0.792, 0.793, 0.794, 0.795, 0.796, 0.797, 0.798, 0.799, 0.800, 0.801, 0.802,
    0.803, 0.804, 0.805, 0.806, 0.807, 0.808, 0.809, 0.810, 0.811, 0.812, 0.813,
    0.814, 0.815, 0.816, 0.817, 0.818, 0.819, 0.820, 0.821, 0.822, 0.823, 0.824,
    0.825, 0.826, 0.827, 0.828, 0.829, 0.830, 0.831, 0.832, 0.833, 0.834, 0.835,
    0.836, 0.837, 0.838, 0.839, 0.840, 0.841, 0.842, 0.843, 0.844, 0.845, 0.846,
    0.847, 0.848, 0.849, 0.850, 0.851, 0.852, 0.853, 0.854, 0.855, 0.856, 0.857,
    0.858, 0.859, 0.860, 0.861, 0.862, 0.863, 0.864, 0.865, 0.866, 0.867, 0.868,
    0.869, 0.870, 0.871, 0.872, 0.873, 0.874, 0.875, 0.876, 0.877, 0.878, 0.879,
    0.880, 0.881, 0.882, 0.883, 0.884, 0.885, 0.886, 0.887, 0.888, 0.889, 0.890,
    0.891, 0.892, 0.893, 0.894, 0.895, 0.896, 0.897, 0.898, 0.899, 0.900, 0.901,
    0.902, 0.903, 0.904, 0.905, 0.906, 0.907, 0.908, 0.909, 0.910, 0.911, 0.912,
    0.913, 0.914, 0.915, 0.916, 0.917, 0.918, 0.919, 0.920, 0.921, 0.922, 0.923,
    0.924, 0.925, 0.926, 0.927, 0.928, 0.929, 0.930, 0.931, 0.932, 0.933, 0.934,
    0.935, 0.936, 0.937, 0.938, 0.939, 0.940, 0.941, 0.942, 0.943, 0.944, 0.945,
    0.946, 0.947, 0.948, 0.949, 0.950, 0.951, 0.952, 0.953, 0.954, 0.955, 0.956,
    0.957, 0.958, 0.959, 0.960, 0.961, 0.962, 0.963, 0.964, 0.965, 0.966, 0.967,
    0.968, 0.969, 0.970, 0.971, 0.972, 0.973, 0.974, 0.975, 0.976, 0.977, 0.978,
    0.979, 0.980, 0.981, 0.982, 0.983, 0.984, 0.985, 0.986, 0.987, 0.988, 0.989,
    0.990, 0.991, 0.992, 0.993, 0.994, 0.995, 0.996, 0.997, 0.998, 0.999, 1.000,
    1.001, 1.002, 1.003, 1.004, 1.005, 1.006, 1.007, 1.008, 1.009, 1.010, 1.011,
    1.012, 1.013, 1.014, 1.015, 1.016, 1.017, 1.018, 1.019, 1.020, 1.021, 1.022,
    1.023, 1.024, 1.025, 1.026, 1.027, 1.028, 1.029, 1.030, 1.031, 1.032, 1.033,
    1.034, 1.035, 1.036, 1.037, 1.038, 1.039, 1.040, 1.041, 1.042, 1.043, 1.044,
    1.045, 1.046, 1.047, 1.048, 1.049, 1.050, 1.051, 1.052, 1.053, 1.054, 1.055,
    1.056, 1.057, 1.058, 1.059, 1.060, 1.061, 1.062, 1.063, 1.064, 1.065, 1.066,
    1.067, 1.068, 1.069, 1.070, 1.071, 1.072, 1.073, 1.074, 1.075, 1.076, 1.077,
    1.078, 1.079, 1.080, 1.081, 1.082, 1.083, 1.084, 1.085, 1.086, 1.087, 1.088,
    1.089, 1.090, 1.091, 1.092, 1.093, 1.094, 1.095, 1.096, 1.097, 1.098, 1.099,
    1.100, 1.101, 1.102, 1.103, 1.104, 1.105, 1.106, 1.107, 1.108, 1.109, 1.110,
    1.111, 1.112, 1.113, 1.114, 1.115, 1.116, 1.117, 1.118, 1.119, 1.120, 1.121,
    1.122, 1.123, 1.124, 1.125, 1.126, 1.127, 1.128, 1.129, 1.130, 1.131, 1.132,
    1.133, 1.134, 1.135, 1.136, 1.137, 1.138, 1.139, 1.140, 1.141, 1.142, 1.143,
    1.144, 1.145, 1.146, 1.147, 1.148, 1.149, 1.150, 1.151, 1.152, 1.153, 1.154,
    1.155, 1.156, 1.157, 1.158, 1.159, 1.160, 1.161, 1.162, 1.163, 1.164, 1.165,
    1.166, 1.167, 1.168, 1.169, 1.170, 1.171, 1.172, 1.173, 1.174, 1.175, 1.176,
    1.177, 1.178, 1.179, 1.180, 1.181, 1.182, 1.183, 1.184, 1.185, 1.186, 1.187,
    1.188, 1.189, 1.190, 1.191, 1.192, 1.193, 1.194, 1.195, 1.196, 1.197, 1.198,
    1.199, 1.200, 1.201, 1.202, 1.203, 1.204, 1.205, 1.206, 1.207, 1.208, 1.209,
    1.210, 1.211, 1.212, 1.213, 1.214, 1.215, 1.216, 1.217, 1.218, 1.219, 1.220,
    1.221, 1.222, 1.223, 1.224, 1.225, 1.226, 1.227, 1.228, 1.229, 1.230, 1.231,
    1.232, 1.233, 1.234, 1.235, 1.236, 1.237, 1.238, 1.239, 1.240, 1.241, 1.242,
    1.243, 1.244, 1.245, 1.246, 1.247, 1.248, 1.249, 1.250, 1.251, 1.252, 1.253,
    1.254, 1.255, 1.256, 1.257, 1.258, 1.259, 1.260, 1.261, 1.262, 1.263, 1.264,
    1.265, 1.266, 1.267, 1.268, 1.269, 1.270, 1.271, 1.272, 1.273, 1.274, 1.275,
    1.276, 1.277, 1.278, 1.279, 1.280, 1.281, 1.282, 1.283, 1.284, 1.285, 1.286,
    1.287, 1.288, 1.289, 1.290, 1.291, 1.292, 1.293, 1.294, 1.295, 1.296, 1.297,
    1.298, 1.299, 1.300, 1.301, 1.302, 1.303, 1.304, 1.305, 1.306, 1.307, 1.308,
    1.309, 1.310, 1.311, 1.312, 1.313, 1.314, 1.315, 1.316, 1.317, 1.318, 1.319,
    1.320, 1.321, 1.322, 1.323, 1.324, 1.325, 1.326, 1.327, 1.328, 1.329, 1.330,
    1.331, 1.332, 1.333, 1.334, 1.335, 1.336, 1.337, 1.338, 1.339, 1.340, 1.341,
    1.342, 1.343, 1.344, 1.345, 1.346, 1.347, 1.348, 1.349, 1.350, 1.351, 1.352,
    1.353, 1.354, 1.355, 1.356, 1.357, 1.358, 1.359, 1.360, 1.361, 1.362, 1.363,
    1.364, 1.365, 1.366, 1.367, 1.368, 1.369, 1.370, 1.371, 1.372, 1.373, 1.374,
    1.375, 1.376, 1.377, 1.378, 1.379, 1.380, 1.381, 1.382, 1.383, 1.384, 1.385,
    1.386, 1.387, 1.388, 1.389, 1.390, 1.391, 1.392, 1.393, 1.394, 1.395, 1.396,
    1.397, 1.398, 1.399, 1.400, 1.401, 1.402, 1.403, 1.404, 1.405, 1.406, 1.407,
    1.408, 1.409, 1.410, 1.411, 1.412, 1.413, 1.414, 1.415, 1.416, 1.417, 1.418,
    1.419, 1.420, 1.421, 1.422, 1.423, 1.424, 1.425, 1.426, 1.427, 1.428, 1.429,
    1.430, 1.431, 1.432, 1.433, 1.434, 1.435, 1.436, 1.437, 1.438, 1.439, 1.440,
    1.441, 1.442, 1.443, 1.444, 1.445, 1.446, 1.447, 1.448, 1.449, 1.450, 1.451,
    1.452, 1.453, 1.454, 1.455, 1.456, 1.457, 1.458, 1.459, 1.460, 1.461, 1.462,
    1.463, 1.464, 1.465, 1.466, 1.467, 1.468, 1.469, 1.470, 1.471, 1.472, 1.473,
    1.474, 1.475, 1.476, 1.477, 1.478, 1.479, 1.480, 1.481, 1.482, 1.483, 1.484,
    1.485, 1.486, 1.487, 1.488, 1.489, 1.490, 1.491, 1.492, 1.493, 1.494, 1.495,
    1.496, 1.497, 1.498, 1.499, 1.500, 1.501, 1.502, 1.503, 1.504, 1.505, 1.506,
    1.507, 1.508, 1.509, 1.510, 1.511, 1.512, 1.513, 1.514, 1.515, 1.516, 1.517,
    1.518, 1.519, 1.520, 1.521, 1.522, 1.523, 1.524, 1.525, 1.526, 1.527, 1.528,
    1.529, 1.530, 1.531, 1.532, 1.533, 1.534, 1.535, 1.536, 1.537, 1.538, 1.539,
    1.540, 1.541, 1.542, 1.543, 1.544, 1.545, 1.546, 1.547, 1.548, 1.549, 1.550,
    1.551, 1.552, 1.553, 1.554, 1.555, 1.556, 1.557, 1.558, 1.559, 1.560, 1.561,
    1.562, 1.563, 1.564, 1.565, 1.566, 1.567, 1.568, 1.569, 1.570, 1.571, 1.572,
    1.573, 1.574, 1.575, 1.576, 1.577, 1.578, 1.579, 1.580, 1.581, 1.582, 1.583,
    1.584, 1.585, 1.586, 1.587, 1.588, 1.589, 1.590, 1.591, 1.592, 1.593, 1.594,
    1.595, 1.596, 1.597, 1.598, 1.599, 1.600, 1.601, 1.602, 1.603, 1.604, 1.605,
    1.606, 1.607, 1.608, 1.609, 1.610, 1.611, 1.612, 1.613, 1.614, 1.615, 1.616,
    1.617, 1.618, 1.619, 1.620, 1.621, 1.622, 1.623, 1.624, 1.625, 1.626, 1.627,
    1.628, 1.629, 1.630, 1.631, 1.632, 1.633, 1.634, 1.635, 1.636, 1.637, 1.638,
    1.639, 1.640, 1.641, 1.642, 1.643, 1.644, 1.645, 1.646, 1.647, 1.648, 1.649,
    1.650, 1.651, 1.652, 1.653, 1.654, 1.655, 1.656, 1.657, 1.658, 1.659, 1.660,
    1.661, 1.662, 1.663, 1.664, 1.665, 1.666, 1.667, 1.668, 1.669, 1.670, 1.671,
    1.672, 1.673, 1.674, 1.675, 1.676, 1.677, 1.678, 1.679, 1.680, 1.681, 1.682,
    1.683, 1.684, 1.685, 1.686, 1.687, 1.688, 1.689, 1.690, 1.691, 1.692, 1.693,
    1.694, 1.695, 1.696, 1.697, 1.698, 1.699, 1.700, 1.701, 1.702, 1.703, 1.704,
    1.705, 1.706, 1.707, 1.708, 1.709, 1.710, 1.711, 1.712, 1.713, 1.714, 1.715,
    1.716, 1.717, 1.718, 1.719, 1.720, 1.721, 1.722, 1.723, 1.724, 1.725, 1.726,
    1.727, 1.728, 1.729, 1.730, 1.731, 1.732, 1.733, 1.734, 1.735, 1.736, 1.737,
    1.738, 1.739, 1.740, 1.741, 1.742, 1.743, 1.744, 1.745, 1.746, 1.747, 1.748,
    1.749, 1.750, 1.751, 1.752, 1.753, 1.754, 1.755, 1.756, 1.757, 1.758, 1.759,
    1.760, 1.761, 1.762, 1.763, 1.764, 1.765, 1.766, 1.767, 1.768, 1.769, 1.770,
    1.771, 1.772, 1.773, 1.774, 1.775, 1.776, 1.777, 1.778, 1.779, 1.780, 1.781,
    1.782, 1.783, 1.784, 1.785, 1.786, 1.787, 1.788, 1.789, 1.790, 1.791, 1.792,
    1.793, 1.794, 1.795, 1.796, 1.797, 1.798, 1.799, 1.800, 1.801, 1.802, 1.803,
    1.804, 1.805, 1.806, 1.807, 1.808, 1.809, 1.810, 1.811, 1.812, 1.813, 1.814,
    1.815, 1.816, 1.817, 1.818, 1.819, 1.820, 1.821, 1.822, 1.823, 1.824, 1.825,
    1.826, 1.827, 1.828, 1.829, 1.830, 1.831, 1.832, 1.833, 1.834, 1.835, 1.836,
    1.837, 1.838, 1.839, 1.840, 1.841, 1.842, 1.843, 1.844, 1.845, 1.846, 1.847,
    1.848, 1.849, 1.850, 1.851, 1.852, 1.853, 1.854, 1.855, 1.856, 1.857, 1.858,
    1.859, 1.860, 1.861, 1.862, 1.863, 1.864, 1.865, 1.866, 1.867, 1.868, 1.869,
    1.870, 1.871, 1.872, 1.873, 1.874, 1.875, 1.876, 1.877, 1.878, 1.879, 1.880,
    1.881, 1.882, 1.883, 1.884, 1.885, 1.886, 1.887, 1.888, 1.889, 1.890, 1.891,
    1.892, 1.893, 1.894, 1.895, 1.896, 1.897, 1.898, 1.899, 1.900, 1.901, 1.902,
    1.903, 1.904, 1.905, 1.906, 1.907, 1.908, 1.909, 1.910, 1.911, 1.912, 1.913,
    1.914, 1.915, 1.916, 1.917, 1.918, 1.919, 1.920, 1.921, 1.922, 1.923, 1.924,
    1.925, 1.926, 1.927, 1.928, 1.929, 1.930, 1.931, 1.932, 1.933, 1.934, 1.935,
    1.936, 1.937, 1.938, 1.939, 1.940, 1.941, 1.942, 1.943, 1.944, 1.945, 1.946,
    1.947, 1.948, 1.949, 1.950, 1.951, 1.952, 1.953, 1.954, 1.955, 1.956, 1.957,
    1.958, 1.959, 1.960, 1.961, 1.962, 1.963, 1.964, 1.965, 1.966, 1.967, 1.968,
    1.969, 1.970, 1.971, 1.972, 1.973, 1.974, 1.975, 1.976, 1.977, 1.978, 1.979,
    1.980, 1.981, 1.982, 1.983, 1.984, 1.985, 1.986, 1.987, 1.988, 1.989, 1.990,
    1.991, 1.992, 1.993, 1.994, 1.995, 1.996, 1.997, 1.998, 1.999, 2.000, 2.001,
    2.002, 2.003, 2.004, 2.005, 2.006, 2.007, 2.008, 2.009, 2.010, 2.011, 2.012,
    2.013, 2.014, 2.015, 2.016, 2.017, 2.018, 2.019, 2.020, 2.021, 2.022, 2.023,
    2.024, 2.025, 2.026, 2.027, 2.028, 2.029, 2.030, 2.031, 2.032, 2.033, 2.034,
    2.035, 2.036, 2.037, 2.038, 2.039, 2.040, 2.041, 2.042, 2.043, 2.044, 2.045,
    2.046, 2.047, 2.048, 2.049, 2.050, 2.051, 2.052, 2.053, 2.054, 2.055, 2.056,
    2.057, 2.058, 2.059, 2.060, 2.061, 2.062, 2.063, 2.064, 2.065, 2.066, 2.067,
    2.068, 2.069, 2.070, 2.071, 2.072, 2.073, 2.074, 2.075, 2.076, 2.077, 2.078,
    2.079, 2.080, 2.081, 2.082, 2.083, 2.084, 2.085, 2.086, 2.087, 2.088, 2.089,
    2.090, 2.091, 2.092, 2.093, 2.094, 2.095, 2.096, 2.097, 2.098, 2.099, 2.100,
    2.101, 2.102, 2.103, 2.104, 2.105, 2.106, 2.107, 2.108, 2.109, 2.110, 2.111,
    2.112, 2.113, 2.114, 2.115, 2.116, 2.117, 2.118, 2.119, 2.120, 2.121, 2.122,
    2.123, 2.124, 2.125, 2.126, 2.127, 2.128, 2.129, 2.130, 2.131, 2.132, 2.133,
    2.134, 2.135, 2.136, 2.137, 2.138, 2.139, 2.140, 2.141, 2.142, 2.143, 2.144,
    2.145, 2.146, 2.147, 2.148, 2.149, 2.150, 2.151, 2.152, 2.153, 2.154, 2.155,
    2.156, 2.157, 2.158, 2.159, 2.160, 2.161, 2.162, 2.163, 2.164, 2.165, 2.166,
    2.167, 2.168, 2.169, 2.170, 2.171, 2.172, 2.173, 2.174, 2.175, 2.176, 2.177,
    2.178, 2.179, 2.180, 2.181, 2.182, 2.183, 2.184, 2.185, 2.186, 2.187, 2.188,
    2.189, 2.190, 2.191, 2.192, 2.193, 2.194, 2.195, 2.196, 2.197, 2.198, 2.199,
    2.200, 2.201, 2.202, 2.203, 2.204, 2.205, 2.206, 2.207, 2.208, 2.209, 2.210,
    2.211, 2.212, 2.213, 2.214, 2.215, 2.216, 2.217, 2.218, 2.219, 2.220, 2.221,
    2.222, 2.223, 2.224, 2.225, 2.226, 2.227, 2.228, 2.229, 2.230, 2.231, 2.232,
    2.233, 2.234, 2.235, 2.236, 2.237, 2.238, 2.239, 2.240, 2.241, 2.242, 2.243,
    2.244, 2.245, 2.246, 2.247, 2.248, 2.249, 2.250, 2.251, 2.252, 2.253, 2.254,
    2.255, 2.256, 2.257, 2.258, 2.259, 2.260, 2.261, 2.262, 2.263, 2.264, 2.265,
    2.266, 2.267, 2.268, 2.269, 2.270, 2.271, 2.272, 2.273, 2.274, 2.275, 2.276,
    2.277, 2.278, 2.279, 2.280, 2.281, 2.282, 2.283, 2.284, 2.285, 2.286, 2.287,
    2.288, 2.289, 2.290, 2.291, 2.292, 2.293, 2.294, 2.295, 2.296, 2.297, 2.298,
    2.299, 2.300, 2.301, 2.302, 2.303, 2.304, 2.305, 2.306, 2.307, 2.308, 2.309,
    2.310, 2.311, 2.312, 2.313, 2.314, 2.315, 2.316, 2.317, 2.318, 2.319, 2.320,
    2.321, 2.322, 2.323, 2.324, 2.325, 2.326, 2.327, 2.328, 2.329, 2.330, 2.331,
    2.332, 2.333, 2.334, 2.335, 2.336, 2.337, 2.338, 2.339, 2.340, 2.341, 2.342,
    2.343, 2.344, 2.345, 2.346, 2.347, 2.348, 2.349, 2.350, 2.351, 2.352, 2.353,
    2.354, 2.355, 2.356, 2.357, 2.358, 2.359, 2.360, 2.361, 2.362, 2.363, 2.364,
    2.365, 2.366, 2.367, 2.368, 2.369, 2.370, 2.371, 2.372, 2.373, 2.374, 2.375,
    2.376, 2.377, 2.378, 2.379, 2.380, 2.381, 2.382, 2.383, 2.384, 2.385, 2.386,
    2.387, 2.388, 2.389, 2.390, 2.391, 2.392, 2.393, 2.394, 2.395, 2.396, 2.397,
    2.398, 2.399, 2.400, 2.401, 2.402, 2.403, 2.404, 2.405, 2.406, 2.407, 2.408,
    2.409, 2.410, 2.411, 2.412, 2.413, 2.414, 2.415, 2.416, 2.417, 2.418, 2.419,
    2.420, 2.421, 2.422, 2.423, 2.424, 2.425, 2.426, 2.427, 2.428, 2.429, 2.430,
    2.431, 2.432, 2.433, 2.434, 2.435, 2.436, 2.437, 2.438, 2.439, 2.440, 2.441,
    2.442, 2.443, 2.444, 2.445, 2.446, 2.447, 2.448, 2.449, 2.450, 2.451, 2.452,
    2.453, 2.454, 2.455, 2.456, 2.457, 2.458, 2.459, 2.460, 2.461, 2.462, 2.463,
    2.464, 2.465, 2.466, 2.467, 2.468, 2.469, 2.470, 2.471, 2.472, 2.473, 2.474,
    2.475, 2.476, 2.477, 2.478, 2.479, 2.480, 2.481, 2.482, 2.483, 2.484, 2.485,
    2.486, 2.487, 2.488, 2.489, 2.490, 2.491, 2.492, 2.493, 2.494, 2.495, 2.496,
    2.497, 2.498, 2.499, 2.500, 2.501, 2.502, 2.503, 2.504, 2.505, 2.506, 2.507,
    2.508, 2.509, 2.510, 2.511, 2.512, 2.513, 2.514, 2.515, 2.516, 2.517, 2.518,
    2.519, 2.520, 2.521, 2.522, 2.523, 2.524, 2.525, 2.526, 2.527, 2.528, 2.529,
    2.530, 2.531, 2.532, 2.533, 2.534, 2.535, 2.536, 2.537, 2.538, 2.539, 2.540,
    2.541, 2.542, 2.543, 2.544, 2.545, 2.546, 2.547, 2.548, 2.549, 2.550, 2.551,
    2.552, 2.553, 2.554, 2.555, 2.556, 2.557, 2.558, 2.559, 2.560, 2.561, 2.562,
    2.563, 2.564, 2.565, 2.566, 2.567, 2.568, 2.569, 2.570, 2.571, 2.572, 2.573,
    2.574, 2.575, 2.576, 2.577, 2.578, 2.579, 2.580, 2.581, 2.582, 2.583, 2.584,
    2.585, 2.586, 2.587, 2.588, 2.589, 2.590, 2.591, 2.592, 2.593, 2.594, 2.595,
    2.596, 2.597, 2.598, 2.599, 2.600, 2.601, 2.602, 2.603, 2.604, 2.605, 2.606,
    2.607, 2.608, 2.609, 2.610, 2.611, 2.612, 2.613, 2.614, 2.615, 2.616, 2.617,
    2.618, 2.619, 2.620, 2.621, 2.622, 2.623, 2.624, 2.625, 2.626, 2.627, 2.628,
    2.629, 2.630, 2.631, 2.632, 2.633, 2.634, 2.635, 2.636, 2.637, 2.638, 2.639,
    2.640, 2.641, 2.642, 2.643, 2.644, 2.645, 2.646, 2.647, 2.648, 2.649, 2.650,
    2.651, 2.652, 2.653, 2.654, 2.655, 2.656, 2.657, 2.658, 2.659, 2.660, 2.661,
    2.662, 2.663, 2.664, 2.665, 2.666, 2.667, 2.668, 2.669, 2.670, 2.671, 2.672,
    2.673, 2.674, 2.675, 2.676, 2.677, 2.678, 2.679, 2.680, 2.681, 2.682, 2.683,
    2.684, 2.685, 2.686, 2.687, 2.688, 2.689, 2.690, 2.691, 2.692, 2.693, 2.694,
    2.695, 2.696, 2.697, 2.698, 2.699, 2.700, 2.701, 2.702, 2.703, 2.704, 2.705,
    2.706, 2.707, 2.708, 2.709, 2.710, 2.711, 2.712, 2.713, 2.714, 2.715, 2.716,
    2.717, 2.718, 2.719, 2.720, 2.721, 2.722, 2.723, 2.724, 2.725, 2.726, 2.727,
    2.728, 2.729, 2.730, 2.731, 2.732, 2.733, 2.734, 2.735, 2.736, 2.737, 2.738,
    2.739, 2.740, 2.741, 2.742, 2.743, 2.744, 2.745, 2.746, 2.747, 2.748, 2.749,
    2.750, 2.751, 2.752, 2.753, 2.754, 2.755, 2.756, 2.757, 2.758, 2.759, 2.760,
    2.761, 2.762, 2.763, 2.764, 2.765, 2.766, 2.767, 2.768, 2.769, 2.770, 2.771,
    2.772, 2.773, 2.774, 2.775, 2.776, 2.777, 2.778, 2.779, 2.780, 2.781, 2.782,
    2.783, 2.784, 2.785, 2.786, 2.787, 2.788, 2.789, 2.790, 2.791, 2.792, 2.793,
    2.794, 2.795, 2.796, 2.797, 2.798, 2.799, 2.800, 2.801, 2.802, 2.803, 2.804,
    2.805, 2.806, 2.807, 2.808, 2.809, 2.810, 2.811, 2.812, 2.813, 2.814, 2.815,
    2.816, 2.817, 2.818, 2.819, 2.820, 2.821, 2.822, 2.823, 2.824, 2.825, 2.826,
    2.827, 2.828, 2.829, 2.830, 2.831, 2.832, 2.833, 2.834, 2.835, 2.836, 2.837,
    2.838, 2.839, 2.840, 2.841, 2.842, 2.843, 2.844, 2.845, 2.846, 2.847, 2.848,
    2.849, 2.850, 2.851, 2.852, 2.853, 2.854, 2.855, 2.856, 2.857, 2.858, 2.859,
    2.860, 2.861, 2.862, 2.863, 2.864, 2.865, 2.866, 2.867, 2.868, 2.869, 2.870,
    2.871, 2.872, 2.873, 2.874, 2.875, 2.876, 2.877, 2.878, 2.879, 2.880, 2.881,
    2.882, 2.883, 2.884, 2.885, 2.886, 2.887, 2.888, 2.889, 2.890, 2.891, 2.892,
    2.893, 2.894, 2.895, 2.896, 2.897, 2.898, 2.899, 2.900, 2.901, 2.902, 2.903,
    2.904, 2.905, 2.906, 2.907, 2.908, 2.909, 2.910, 2.911, 2.912, 2.913, 2.914,
    2.915, 2.916, 2.917, 2.918, 2.919, 2.920, 2.921, 2.922, 2.923, 2.924, 2.925,
    2.926, 2.927, 2.928, 2.929, 2.930, 2.931, 2.932, 2.933, 2.934, 2.935, 2.936,
    2.937, 2.938, 2.939, 2.940, 2.941, 2.942, 2.943, 2.944, 2.945, 2.946, 2.947,
    2.948, 2.949, 2.950, 2.951, 2.952, 2.953, 2.954, 2.955, 2.956, 2.957, 2.958,
    2.959, 2.960, 2.961, 2.962, 2.963, 2.964, 2.965, 2.966, 2.967, 2.968, 2.969,
    2.970, 2.971, 2.972, 2.973, 2.974, 2.975, 2.976, 2.977, 2.978, 2.979, 2.980,
    2.981, 2.982, 2.983, 2.984, 2.985, 2.986, 2.987, 2.988, 2.989, 2.990, 2.991,
    2.992, 2.993, 2.994, 2.995, 2.996, 2.997, 2.998, 2.999, 3.000};

void subTrainLoad(char *txt, float *save_st, int st, int ed, int id,
                  int *sub_num) {
  int i = st;
  int num = 0;
  int signal;
  int cnt = 0;
  while (i <= ed) {
    signal = 1;
    // dbt = 0.0;
    if (*(txt + i) == '-') {
      signal = -1;
      i++;
    }
    float dbt =
        array_result[(*(txt + i + 0) - '0') * 1000 +
                     (*(txt + i + 2) - '0') * 100 +
                     (*(txt + i + 3) - '0') * 10 + (*(txt + i + 4) - '0') * 1];

    cnt++;

    i += 6;  //跳过','
    *(save_st + num++) = signal * dbt;

    if (cnt == 1000) {
      *(save_st + num++) = *(txt + i++) - '0';
      cnt = 0;
      i++;  //跳过'\n'
    }
  }
  *sub_num = num / 1001;
}

bool LR::loadTrainData() {
  char *txt = NULL;
  int fp = open(trainFile.c_str(), O_RDONLY);
  long filesize = lseek(fp, 0, SEEK_END);
  txt = (char *)mmap(NULL, filesize, PROT_READ, MAP_PRIVATE, fp, 0);

  int subtxt_num = 6000 * 5000 / 8;
  int st0 = 0;
  int st1 = subtxt_num;
  int st2 = 2 * subtxt_num;
  int st3 = 3 * subtxt_num;
  int st4 = 4 * subtxt_num;
  int st5 = 5 * subtxt_num;
  int st6 = 6 * subtxt_num;
  int st7 = 7 * subtxt_num;
  int ed = 8 * subtxt_num;
  while (txt[st1] != '\n') st1++;
  while (txt[st2] != '\n') st2++;
  while (txt[st3] != '\n') st3++;
  while (txt[st4] != '\n') st4++;
  while (txt[st5] != '\n') st5++;
  while (txt[st6] != '\n') st6++;
  while (txt[st7] != '\n') st7++;
  while (txt[ed] != '\n') ed--;

  thread pth0(subTrainLoad, txt, subtrain0, st0, st1, 0, &sub_num_0);
  thread pth1(subTrainLoad, txt, subtrain1, st1 + 1, st2, 1, &sub_num_1);
  thread pth2(subTrainLoad, txt, subtrain2, st2 + 1, st3, 2, &sub_num_2);
  thread pth3(subTrainLoad, txt, subtrain3, st3 + 1, st4, 3, &sub_num_3);
  thread pth4(subTrainLoad, txt, subtrain4, st4 + 1, st5, 4, &sub_num_4);
  thread pth5(subTrainLoad, txt, subtrain5, st5 + 1, st6, 5, &sub_num_5);
  thread pth6(subTrainLoad, txt, subtrain6, st6 + 1, st7, 6, &sub_num_6);
  thread pth7(subTrainLoad, txt, subtrain7, st7 + 1, ed, 7, &sub_num_7);
  pth0.join();
  pth1.join();
  pth2.join();
  pth3.join();
  pth4.join();
  pth5.join();
  pth6.join();
  pth7.join();

  int sub_num_last = train_num - sub_num_0 - sub_num_1 - sub_num_2 - sub_num_3 -
                     sub_num_4 - sub_num_5 - sub_num_6 - sub_num_7;
  int sub_num = 1001 * sub_num_last;
  int i = ed + 1;
  int num = 0;
  int signal;
  int cnt = 0;
  while (i <= filesize) {
    signal = 1;
    if (*(txt + i) == '-') {
      signal = -1;
      i++;
    }
    float dbt =
        array_result[(*(txt + i + 0) - '0') * 1000 +
                     (*(txt + i + 2) - '0') * 100 +
                     (*(txt + i + 3) - '0') * 10 + (*(txt + i + 4) - '0') * 1];
    cnt++;

    i += 6;  //跳过','
    *(subtrain7 + sub_num_7 * 1001 + num++) = signal * dbt;

    if (cnt == 1000) {
      *(subtrain7 + sub_num_7 * 1001 + num++) = *(txt + i++) - '0';
      cnt = 0;
      i++;  //跳过'\n'
    }
    if (num == sub_num) break;
  }
  sub_num_7 += sub_num_last;

  return true;
}

void LR::initParam() {
  int i;
  for (i = 0; i < featuresNum; i += 4) {
    *(param + i) = wtInitV;
    *(param + i + 1) = wtInitV;
    *(param + i + 2) = wtInitV;
    *(param + i + 3) = wtInitV;
  }
}

bool LR::init() {
  predictVec = (int *)(malloc(30000 * sizeof(int)));
  param = (float *)(malloc((featuresNum + 1) * sizeof(float)));
  bool status = loadTrainData();
  if (status != true) {
    return false;
  }
  initParam();
  return true;
}

float LR::wxbCalcTrain(float *sub_train, const int index) {
  float mulSum = 0.0L;
  int i;
  float wtv, feav;
  float32x4_t sum_vec = vdupq_n_f32(0.0), wtv_vec,
              feav_vec;  //定义用于暂存累加结果的寄存器且初始化为0
  for (i = 0; i < featuresNum; i += 4) {
    wtv_vec = vld1q_f32(param + i);
    feav_vec = vld1q_f32(sub_train + index * (featuresNum + 1) + i);
    sum_vec = vmlaq_f32(sum_vec, wtv_vec, feav_vec);
  }
  float32x2_t r = vadd_f32(vget_high_f32(sum_vec), vget_low_f32(sum_vec));
  mulSum += vget_lane_f32(vpadd_f32(r, r), 0);
  return mulSum;
}

float sigmoidCalc(const float wxb) {
  float expv = exp(-1 * wxb);
  float expvInv = 1 / (1 + expv);
  return expvInv;
}

void LR::train() {
  int batch_num = (train_num / batch_size) + 1;
  int sub_batch_0 = sub_num_0 / batch_size;
  int sub_last_0 = sub_num_0 % batch_size;
  int sub_batch_1 = (sub_num_1 - (8 - sub_last_0)) / batch_size;
  int sub_last_1 = (sub_num_1 - (8 - sub_last_0)) % batch_size;
  int sub_batch_2 = (sub_num_2 - (8 - sub_last_1)) / batch_size;
  int sub_last_2 = (sub_num_2 - (8 - sub_last_1)) % batch_size;
  int sub_batch_3 = (sub_num_3 - (8 - sub_last_2)) / batch_size;
  int sub_last_3 = (sub_num_3 - (8 - sub_last_2)) % batch_size;

  int sub_batch_4 = (sub_num_4 - (8 - sub_last_3)) / batch_size;
  int sub_last_4 = (sub_num_4 - (8 - sub_last_3)) % batch_size;
  int sub_batch_5 = (sub_num_5 - (8 - sub_last_4)) / batch_size;
  int sub_last_5 = (sub_num_5 - (8 - sub_last_4)) % batch_size;
  int sub_batch_6 = (sub_num_6 - (8 - sub_last_5)) / batch_size;
  int sub_last_6 = (sub_num_6 - (8 - sub_last_5)) % batch_size;
  int sub_batch_7 = (sub_num_7 - (8 - sub_last_6)) / batch_size;
  int sub_last_7 = (sub_num_7 - (8 - sub_last_6)) % batch_size;

  int start = 0, end = 0;
  // 训练子集0
  for (int i = 0; i < sub_batch_0; i++) {
    start = i * batch_size;
    end = start + batch_size;

    float gsV[1000] = {0};
    float32x4_t left_vec, right_vec, sum_vec;
    for (int j = start; j < end; j++) {
      float wxbVal = wxbCalcTrain(subtrain0, j);
      float expvInv = 1 / (1 + exp(-1 * wxbVal));
      float sigv =
          expvInv - (int)(*(subtrain0 + j * (featuresNum + 1) + featuresNum));
      left_vec = vdupq_n_f32(sigv);
      for (int k = 0; k < featuresNum; k += 4) {
        sum_vec = vld1q_f32(gsV + k);
        right_vec = vld1q_f32(subtrain0 + j * (featuresNum + 1) + k);
        sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
        vst1q_f32(gsV + k, sum_vec);
      }
    }

    left_vec = vdupq_n_f32(rate);
    for (int j = 0; j < featuresNum; j += 4) {
      sum_vec = vld1q_f32(param + j);
      right_vec = vld1q_f32(gsV + j);
      sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(param + j, sum_vec);
    }
  }

  // 训练子集0和1交界
  start = end;
  end = sub_num_0;
  float gsV[1000] = {0};
  float32x4_t left_vec, right_vec, sum_vec;
  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain0, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain0 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV + k);
      right_vec = vld1q_f32(subtrain0 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV + k, sum_vec);
    }
  }

  start = 0;
  end = 8 - sub_last_0;
  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain1, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain1 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV + k);
      right_vec = vld1q_f32(subtrain1 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV + k, sum_vec);
    }
  }

  left_vec = vdupq_n_f32(rate);
  for (int j = 0; j < featuresNum; j += 4) {
    sum_vec = vld1q_f32(param + j);
    right_vec = vld1q_f32(gsV + j);
    sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
    vst1q_f32(param + j, sum_vec);
  }

  // 训练子集1
  for (int i = 0; i < sub_batch_1; i++) {
    start = i * batch_size + 8 - sub_last_0;
    end = start + batch_size;

    float gsV[1000] = {0};
    float32x4_t left_vec, right_vec, sum_vec;
    for (int j = start; j < end; j++) {
      float wxbVal = wxbCalcTrain(subtrain1, j);
      float expvInv = 1 / (1 + exp(-1 * wxbVal));
      float sigv =
          expvInv - (int)(*(subtrain1 + j * (featuresNum + 1) + featuresNum));
      left_vec = vdupq_n_f32(sigv);
      for (int k = 0; k < featuresNum; k += 4) {
        sum_vec = vld1q_f32(gsV + k);
        right_vec = vld1q_f32(subtrain1 + j * (featuresNum + 1) + k);
        sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
        vst1q_f32(gsV + k, sum_vec);
      }
    }

    left_vec = vdupq_n_f32(rate);
    for (int j = 0; j < featuresNum; j += 4) {
      sum_vec = vld1q_f32(param + j);
      right_vec = vld1q_f32(gsV + j);
      sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(param + j, sum_vec);
    }
  }

  // 训练子集1和2交界
  start = end;
  end = sub_num_1;
  float gsV1[1000] = {0};
  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain1, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain1 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV1 + k);
      right_vec = vld1q_f32(subtrain1 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV1 + k, sum_vec);
    }
  }

  start = 0;
  end = 8 - sub_last_1;
  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain2, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain2 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV1 + k);
      right_vec = vld1q_f32(subtrain2 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV1 + k, sum_vec);
    }
  }
  left_vec = vdupq_n_f32(rate);
  for (int j = 0; j < featuresNum; j += 4) {
    sum_vec = vld1q_f32(param + j);
    right_vec = vld1q_f32(gsV1 + j);
    sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
    vst1q_f32(param + j, sum_vec);
  }

  // 训练子集2
  for (int i = 0; i < sub_batch_2; i++) {
    start = i * batch_size + 8 - sub_last_1;
    end = start + batch_size;

    float gsV[1000] = {0};
    float32x4_t left_vec, right_vec, sum_vec;
    for (int j = start; j < end; j++) {
      float wxbVal = wxbCalcTrain(subtrain2, j);
      float expvInv = 1 / (1 + exp(-1 * wxbVal));
      float sigv =
          expvInv - (int)(*(subtrain2 + j * (featuresNum + 1) + featuresNum));
      left_vec = vdupq_n_f32(sigv);
      for (int k = 0; k < featuresNum; k += 4) {
        sum_vec = vld1q_f32(gsV + k);
        right_vec = vld1q_f32(subtrain2 + j * (featuresNum + 1) + k);
        sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
        vst1q_f32(gsV + k, sum_vec);
      }
    }

    left_vec = vdupq_n_f32(rate);
    for (int j = 0; j < featuresNum; j += 4) {
      sum_vec = vld1q_f32(param + j);
      right_vec = vld1q_f32(gsV + j);
      sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(param + j, sum_vec);
    }
  }

  // 训练子集2和3交界
  start = end;
  end = sub_num_2;
  float gsV2[1000] = {0};
  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain2, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain2 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV2 + k);
      right_vec = vld1q_f32(subtrain2 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV2 + k, sum_vec);
    }
  }

  start = 0;
  end = 8 - sub_last_2;
  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain3, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain3 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV2 + k);
      right_vec = vld1q_f32(subtrain3 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV2 + k, sum_vec);
    }
  }

  left_vec = vdupq_n_f32(rate);
  for (int j = 0; j < featuresNum; j += 4) {
    sum_vec = vld1q_f32(param + j);
    right_vec = vld1q_f32(gsV2 + j);
    sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
    vst1q_f32(param + j, sum_vec);
  }

  // 训练子集3
  for (int i = 0; i < sub_batch_3; i++) {
    start = i * batch_size + 8 - sub_last_2;
    end = start + batch_size;

    float gsV[1000] = {0};
    float32x4_t left_vec, right_vec, sum_vec;
    for (int j = start; j < end; j++) {
      float wxbVal = wxbCalcTrain(subtrain3, j);
      float expvInv = 1 / (1 + exp(-1 * wxbVal));
      float sigv =
          expvInv - (int)(*(subtrain3 + j * (featuresNum + 1) + featuresNum));
      left_vec = vdupq_n_f32(sigv);
      for (int k = 0; k < featuresNum; k += 4) {
        sum_vec = vld1q_f32(gsV + k);
        right_vec = vld1q_f32(subtrain3 + j * (featuresNum + 1) + k);
        sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
        vst1q_f32(gsV + k, sum_vec);
      }
    }

    left_vec = vdupq_n_f32(rate);
    for (int j = 0; j < featuresNum; j += 4) {
      sum_vec = vld1q_f32(param + j);
      right_vec = vld1q_f32(gsV + j);
      sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(param + j, sum_vec);
    }
  }

  // 训练子集3和4交界
  start = end;
  end = sub_num_3;
  float gsV3[1000] = {0};
  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain3, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain3 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV3 + k);
      right_vec = vld1q_f32(subtrain3 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV3 + k, sum_vec);
    }
  }

  start = 0;
  end = 8 - sub_last_3;
  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain4, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain4 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV3 + k);
      right_vec = vld1q_f32(subtrain4 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV3 + k, sum_vec);
    }
  }

  left_vec = vdupq_n_f32(rate);
  for (int j = 0; j < featuresNum; j += 4) {
    sum_vec = vld1q_f32(param + j);
    right_vec = vld1q_f32(gsV3 + j);
    sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
    vst1q_f32(param + j, sum_vec);
  }

  // 训练子集4
  for (int i = 0; i < sub_batch_4; i++) {
    start = i * batch_size + 8 - sub_last_3;
    end = start + batch_size;

    float gsV[1000] = {0};
    float32x4_t left_vec, right_vec, sum_vec;
    for (int j = start; j < end; j++) {
      float wxbVal = wxbCalcTrain(subtrain4, j);
      float expvInv = 1 / (1 + exp(-1 * wxbVal));
      float sigv =
          expvInv - (int)(*(subtrain4 + j * (featuresNum + 1) + featuresNum));
      left_vec = vdupq_n_f32(sigv);
      for (int k = 0; k < featuresNum; k += 4) {
        sum_vec = vld1q_f32(gsV + k);
        right_vec = vld1q_f32(subtrain4 + j * (featuresNum + 1) + k);
        sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
        vst1q_f32(gsV + k, sum_vec);
      }
    }

    left_vec = vdupq_n_f32(rate);
    for (int j = 0; j < featuresNum; j += 4) {
      sum_vec = vld1q_f32(param + j);
      right_vec = vld1q_f32(gsV + j);
      sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(param + j, sum_vec);
    }
  }

  // 训练子集4和5交界
  start = end;
  end = sub_num_4;
  float gsV4[1000] = {0};
  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain4, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain4 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV4 + k);
      right_vec = vld1q_f32(subtrain4 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV4 + k, sum_vec);
    }
  }

  start = 0;
  end = 8 - sub_last_4;

  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain5, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain5 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV4 + k);
      right_vec = vld1q_f32(subtrain5 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV4 + k, sum_vec);
    }
  }

  left_vec = vdupq_n_f32(rate);
  for (int j = 0; j < featuresNum; j += 4) {
    sum_vec = vld1q_f32(param + j);
    right_vec = vld1q_f32(gsV4 + j);
    sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
    vst1q_f32(param + j, sum_vec);
  }

  // 训练子集5
  for (int i = 0; i < sub_batch_5; i++) {
    start = i * batch_size + 8 - sub_last_4;
    end = start + batch_size;

    float gsV[1000] = {0};
    float32x4_t left_vec, right_vec, sum_vec;
    for (int j = start; j < end; j++) {
      float wxbVal = wxbCalcTrain(subtrain5, j);
      float expvInv = 1 / (1 + exp(-1 * wxbVal));
      float sigv =
          expvInv - (int)(*(subtrain5 + j * (featuresNum + 1) + featuresNum));
      left_vec = vdupq_n_f32(sigv);
      for (int k = 0; k < featuresNum; k += 4) {
        sum_vec = vld1q_f32(gsV + k);
        right_vec = vld1q_f32(subtrain5 + j * (featuresNum + 1) + k);
        sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
        vst1q_f32(gsV + k, sum_vec);
      }
    }

    left_vec = vdupq_n_f32(rate);
    for (int j = 0; j < featuresNum; j += 4) {
      sum_vec = vld1q_f32(param + j);
      right_vec = vld1q_f32(gsV + j);
      sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(param + j, sum_vec);
    }
  }

  // 训练子集5和6交界
  start = end;
  end = sub_num_5;
  float gsV5[1000] = {0};

  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain5, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain5 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV5 + k);
      right_vec = vld1q_f32(subtrain5 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV5 + k, sum_vec);
    }
  }

  start = 0;
  end = 8 - sub_last_5;
  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain6, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain6 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV5 + k);
      right_vec = vld1q_f32(subtrain6 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV5 + k, sum_vec);
    }
  }

  left_vec = vdupq_n_f32(rate);
  for (int j = 0; j < featuresNum; j += 4) {
    sum_vec = vld1q_f32(param + j);
    right_vec = vld1q_f32(gsV5 + j);
    sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
    vst1q_f32(param + j, sum_vec);
  }

  // 训练子集6
  for (int i = 0; i < sub_batch_6; i++) {
    start = i * batch_size + 8 - sub_last_5;
    end = start + batch_size;

    float gsV[1000] = {0};
    float32x4_t left_vec, right_vec, sum_vec;
    for (int j = start; j < end; j++) {
      float wxbVal = wxbCalcTrain(subtrain6, j);
      float expvInv = 1 / (1 + exp(-1 * wxbVal));
      float sigv =
          expvInv - (int)(*(subtrain6 + j * (featuresNum + 1) + featuresNum));
      left_vec = vdupq_n_f32(sigv);
      for (int k = 0; k < featuresNum; k += 4) {
        sum_vec = vld1q_f32(gsV + k);
        right_vec = vld1q_f32(subtrain6 + j * (featuresNum + 1) + k);
        sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
        vst1q_f32(gsV + k, sum_vec);
      }
    }

    left_vec = vdupq_n_f32(rate);
    for (int j = 0; j < featuresNum; j += 4) {
      sum_vec = vld1q_f32(param + j);
      right_vec = vld1q_f32(gsV + j);
      sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(param + j, sum_vec);
    }
  }

  // 训练子集6和7交界
  start = end;
  end = sub_num_6;
  float gsV6[1000] = {0};
  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain6, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain6 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV6 + k);
      right_vec = vld1q_f32(subtrain6 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV6 + k, sum_vec);
    }
  }

  start = 0;
  end = 8 - sub_last_6;
  for (int j = start; j < end; j++) {
    float wxbVal = wxbCalcTrain(subtrain7, j);
    float expvInv = 1 / (1 + exp(-1 * wxbVal));
    float sigv =
        expvInv - (int)(*(subtrain7 + j * (featuresNum + 1) + featuresNum));
    left_vec = vdupq_n_f32(sigv);
    for (int k = 0; k < featuresNum; k += 4) {
      sum_vec = vld1q_f32(gsV6 + k);
      right_vec = vld1q_f32(subtrain7 + j * (featuresNum + 1) + k);
      sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(gsV6 + k, sum_vec);
    }
  }

  left_vec = vdupq_n_f32(rate);
  for (int j = 0; j < featuresNum; j += 4) {
    sum_vec = vld1q_f32(param + j);
    right_vec = vld1q_f32(gsV6 + j);
    sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
    vst1q_f32(param + j, sum_vec);
  }

  // 训练子集7
  for (int i = 0; i < sub_batch_7; i++) {
    start = i * batch_size + 8 - sub_last_6;
    if (start >= sub_num_7) break;
    end = start + batch_size;
    if (end >= sub_num_7) end = sub_num_7;

    float gsV[1000] = {0};
    float32x4_t left_vec, right_vec, sum_vec;
    for (int j = start; j < end; j++) {
      float wxbVal = wxbCalcTrain(subtrain7, j);
      float expvInv = 1 / (1 + exp(-1 * wxbVal));
      float sigv =
          expvInv - (int)(*(subtrain7 + j * (featuresNum + 1) + featuresNum));
      left_vec = vdupq_n_f32(sigv);
      for (int k = 0; k < featuresNum; k += 4) {
        sum_vec = vld1q_f32(gsV + k);
        right_vec = vld1q_f32(subtrain7 + j * (featuresNum + 1) + k);
        sum_vec = vmlaq_f32(sum_vec, left_vec, right_vec);
        vst1q_f32(gsV + k, sum_vec);
      }
    }

    left_vec = vdupq_n_f32(rate);
    for (int j = 0; j < featuresNum; j += 4) {
      sum_vec = vld1q_f32(param + j);
      right_vec = vld1q_f32(gsV + j);
      sum_vec = vmlsq_f32(sum_vec, left_vec, right_vec);
      vst1q_f32(param + j, sum_vec);
    }
  }
}

void subTestpredict(char *txt, int *save_st, int st, int ed, int id,
                    float *param) {
  // float array_result[1001] =
  // {0.000,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.010,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.020,0.021,0.022,0.023,0.024,0.025,0.026,0.027,0.028,0.029,0.030,0.031,0.032,0.033,0.034,0.035,0.036,0.037,0.038,0.039,0.040,0.041,0.042,0.043,0.044,0.045,0.046,0.047,0.048,0.049,0.050,0.051,0.052,0.053,0.054,0.055,0.056,0.057,0.058,0.059,0.060,0.061,0.062,0.063,0.064,0.065,0.066,0.067,0.068,0.069,0.070,0.071,0.072,0.073,0.074,0.075,0.076,0.077,0.078,0.079,0.080,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.090,0.091,0.092,0.093,0.094,0.095,0.096,0.097,0.098,0.099,0.100,0.101,0.102,0.103,0.104,0.105,0.106,0.107,0.108,0.109,0.110,0.111,0.112,0.113,0.114,0.115,0.116,0.117,0.118,0.119,0.120,0.121,0.122,0.123,0.124,0.125,0.126,0.127,0.128,0.129,0.130,0.131,0.132,0.133,0.134,0.135,0.136,0.137,0.138,0.139,0.140,0.141,0.142,0.143,0.144,0.145,0.146,0.147,0.148,0.149,0.150,0.151,0.152,0.153,0.154,0.155,0.156,0.157,0.158,0.159,0.160,0.161,0.162,0.163,0.164,0.165,0.166,0.167,0.168,0.169,0.170,0.171,0.172,0.173,0.174,0.175,0.176,0.177,0.178,0.179,0.180,0.181,0.182,0.183,0.184,0.185,0.186,0.187,0.188,0.189,0.190,0.191,0.192,0.193,0.194,0.195,0.196,0.197,0.198,0.199,0.200,0.201,0.202,0.203,0.204,0.205,0.206,0.207,0.208,0.209,0.210,0.211,0.212,0.213,0.214,0.215,0.216,0.217,0.218,0.219,0.220,0.221,0.222,0.223,0.224,0.225,0.226,0.227,0.228,0.229,0.230,0.231,0.232,0.233,0.234,0.235,0.236,0.237,0.238,0.239,0.240,0.241,0.242,0.243,0.244,0.245,0.246,0.247,0.248,0.249,0.250,0.251,0.252,0.253,0.254,0.255,0.256,0.257,0.258,0.259,0.260,0.261,0.262,0.263,0.264,0.265,0.266,0.267,0.268,0.269,0.270,0.271,0.272,0.273,0.274,0.275,0.276,0.277,0.278,0.279,0.280,0.281,0.282,0.283,0.284,0.285,0.286,0.287,0.288,0.289,0.290,0.291,0.292,0.293,0.294,0.295,0.296,0.297,0.298,0.299,0.300,0.301,0.302,0.303,0.304,0.305,0.306,0.307,0.308,0.309,0.310,0.311,0.312,0.313,0.314,0.315,0.316,0.317,0.318,0.319,0.320,0.321,0.322,0.323,0.324,0.325,0.326,0.327,0.328,0.329,0.330,0.331,0.332,0.333,0.334,0.335,0.336,0.337,0.338,0.339,0.340,0.341,0.342,0.343,0.344,0.345,0.346,0.347,0.348,0.349,0.350,0.351,0.352,0.353,0.354,0.355,0.356,0.357,0.358,0.359,0.360,0.361,0.362,0.363,0.364,0.365,0.366,0.367,0.368,0.369,0.370,0.371,0.372,0.373,0.374,0.375,0.376,0.377,0.378,0.379,0.380,0.381,0.382,0.383,0.384,0.385,0.386,0.387,0.388,0.389,0.390,0.391,0.392,0.393,0.394,0.395,0.396,0.397,0.398,0.399,0.400,0.401,0.402,0.403,0.404,0.405,0.406,0.407,0.408,0.409,0.410,0.411,0.412,0.413,0.414,0.415,0.416,0.417,0.418,0.419,0.420,0.421,0.422,0.423,0.424,0.425,0.426,0.427,0.428,0.429,0.430,0.431,0.432,0.433,0.434,0.435,0.436,0.437,0.438,0.439,0.440,0.441,0.442,0.443,0.444,0.445,0.446,0.447,0.448,0.449,0.450,0.451,0.452,0.453,0.454,0.455,0.456,0.457,0.458,0.459,0.460,0.461,0.462,0.463,0.464,0.465,0.466,0.467,0.468,0.469,0.470,0.471,0.472,0.473,0.474,0.475,0.476,0.477,0.478,0.479,0.480,0.481,0.482,0.483,0.484,0.485,0.486,0.487,0.488,0.489,0.490,0.491,0.492,0.493,0.494,0.495,0.496,0.497,0.498,0.499,0.500,0.501,0.502,0.503,0.504,0.505,0.506,0.507,0.508,0.509,0.510,0.511,0.512,0.513,0.514,0.515,0.516,0.517,0.518,0.519,0.520,0.521,0.522,0.523,0.524,0.525,0.526,0.527,0.528,0.529,0.530,0.531,0.532,0.533,0.534,0.535,0.536,0.537,0.538,0.539,0.540,0.541,0.542,0.543,0.544,0.545,0.546,0.547,0.548,0.549,0.550,0.551,0.552,0.553,0.554,0.555,0.556,0.557,0.558,0.559,0.560,0.561,0.562,0.563,0.564,0.565,0.566,0.567,0.568,0.569,0.570,0.571,0.572,0.573,0.574,0.575,0.576,0.577,0.578,0.579,0.580,0.581,0.582,0.583,0.584,0.585,0.586,0.587,0.588,0.589,0.590,0.591,0.592,0.593,0.594,0.595,0.596,0.597,0.598,0.599,0.600,0.601,0.602,0.603,0.604,0.605,0.606,0.607,0.608,0.609,0.610,0.611,0.612,0.613,0.614,0.615,0.616,0.617,0.618,0.619,0.620,0.621,0.622,0.623,0.624,0.625,0.626,0.627,0.628,0.629,0.630,0.631,0.632,0.633,0.634,0.635,0.636,0.637,0.638,0.639,0.640,0.641,0.642,0.643,0.644,0.645,0.646,0.647,0.648,0.649,0.650,0.651,0.652,0.653,0.654,0.655,0.656,0.657,0.658,0.659,0.660,0.661,0.662,0.663,0.664,0.665,0.666,0.667,0.668,0.669,0.670,0.671,0.672,0.673,0.674,0.675,0.676,0.677,0.678,0.679,0.680,0.681,0.682,0.683,0.684,0.685,0.686,0.687,0.688,0.689,0.690,0.691,0.692,0.693,0.694,0.695,0.696,0.697,0.698,0.699,0.700,0.701,0.702,0.703,0.704,0.705,0.706,0.707,0.708,0.709,0.710,0.711,0.712,0.713,0.714,0.715,0.716,0.717,0.718,0.719,0.720,0.721,0.722,0.723,0.724,0.725,0.726,0.727,0.728,0.729,0.730,0.731,0.732,0.733,0.734,0.735,0.736,0.737,0.738,0.739,0.740,0.741,0.742,0.743,0.744,0.745,0.746,0.747,0.748,0.749,0.750,0.751,0.752,0.753,0.754,0.755,0.756,0.757,0.758,0.759,0.760,0.761,0.762,0.763,0.764,0.765,0.766,0.767,0.768,0.769,0.770,0.771,0.772,0.773,0.774,0.775,0.776,0.777,0.778,0.779,0.780,0.781,0.782,0.783,0.784,0.785,0.786,0.787,0.788,0.789,0.790,0.791,0.792,0.793,0.794,0.795,0.796,0.797,0.798,0.799,0.800,0.801,0.802,0.803,0.804,0.805,0.806,0.807,0.808,0.809,0.810,0.811,0.812,0.813,0.814,0.815,0.816,0.817,0.818,0.819,0.820,0.821,0.822,0.823,0.824,0.825,0.826,0.827,0.828,0.829,0.830,0.831,0.832,0.833,0.834,0.835,0.836,0.837,0.838,0.839,0.840,0.841,0.842,0.843,0.844,0.845,0.846,0.847,0.848,0.849,0.850,0.851,0.852,0.853,0.854,0.855,0.856,0.857,0.858,0.859,0.860,0.861,0.862,0.863,0.864,0.865,0.866,0.867,0.868,0.869,0.870,0.871,0.872,0.873,0.874,0.875,0.876,0.877,0.878,0.879,0.880,0.881,0.882,0.883,0.884,0.885,0.886,0.887,0.888,0.889,0.890,0.891,0.892,0.893,0.894,0.895,0.896,0.897,0.898,0.899,0.900,0.901,0.902,0.903,0.904,0.905,0.906,0.907,0.908,0.909,0.910,0.911,0.912,0.913,0.914,0.915,0.916,0.917,0.918,0.919,0.920,0.921,0.922,0.923,0.924,0.925,0.926,0.927,0.928,0.929,0.930,0.931,0.932,0.933,0.934,0.935,0.936,0.937,0.938,0.939,0.940,0.941,0.942,0.943,0.944,0.945,0.946,0.947,0.948,0.949,0.950,0.951,0.952,0.953,0.954,0.955,0.956,0.957,0.958,0.959,0.960,0.961,0.962,0.963,0.964,0.965,0.966,0.967,0.968,0.969,0.970,0.971,0.972,0.973,0.974,0.975,0.976,0.977,0.978,0.979,0.980,0.981,0.982,0.983,0.984,0.985,0.986,0.987,0.988,0.989,0.990,0.991,0.992,0.993,0.994,0.995,0.996,0.997,0.998,0.999,1.000};
  int cnt = 0;
  float *dbt = (float *)malloc(4 * sizeof(float));
  for (int i = st; i < ed; i += 6000) {
    float mulSum = 0.0;
    int num = 0;
    float32x4_t sum_vec = vdupq_n_f32(0.0), wtv_vec,
                feav_vec;  //定义用于暂存累加结果的寄存器且初始化为0
    for (int j = i; j < i + 5999; j += 24) {
      // float dbt = array_result[(*(txt+j+0) - '0')*1000 + (*(txt+j+2) -
      // '0')*100 + (*(txt+j+3) - '0')*10 + (*(txt+j+4) - '0')*1];
      *dbt = array_result[(*(txt + j + 0) - '0') * 1000 +
                          (*(txt + j + 2) - '0') * 100 +
                          (*(txt + j + 3) - '0') * 10 +
                          (*(txt + j + 4) - '0') * 1];
      int num = *(txt + j + 0) * 1000 +
                *(txt + j + 2 * 100 + *(txt + j + 3 * 10) + *(txt + j + 4)) -
                1111 * '0';
      *dbt = array_result[num];

      *(dbt + 1) = array_result[(*(txt + j + 6) - '0') * 1000 +
                                (*(txt + j + 8) - '0') * 100 +
                                (*(txt + j + 9) - '0') * 10 +
                                (*(txt + j + 10) - '0') * 1];
      *(dbt + 2) = array_result[(*(txt + j + 12) - '0') * 1000 +
                                (*(txt + j + 14) - '0') * 100 +
                                (*(txt + j + 15) - '0') * 10 +
                                (*(txt + j + 16) - '0') * 1];
      *(dbt + 3) = array_result[(*(txt + j + 18) - '0') * 1000 +
                                (*(txt + j + 20) - '0') * 100 +
                                (*(txt + j + 21) - '0') * 10 +
                                (*(txt + j + 22) - '0') * 1];
      // mulSum += *(param+num++) * dbt;
      // mulSum += *(param+num++) * dbt1;
      // mulSum += *(param+num++) * dbt2;
      // mulSum += *(param+num++) * dbt3;
      wtv_vec = vld1q_f32(param + num);
      num += 4;
      feav_vec = vld1q_f32(dbt);
      sum_vec = vmlaq_f32(sum_vec, wtv_vec, feav_vec);
    }
    float32x2_t r = vadd_f32(vget_high_f32(sum_vec), vget_low_f32(sum_vec));
    mulSum = vget_lane_f32(vpadd_f32(r, r), 0);

    *(save_st + cnt++) = mulSum >= 0.0 ? 1 : 0;
  }
  // cout  << "test_pthread:  " << id << "   save-st:  " << save_st << "   st:
  // " << st
  // << "   ed:  " << ed << "   num:  " << cnt << endl;
}

void LR::predict() {
  char *txt = NULL;
  int fp = open(testFile.c_str(), O_RDONLY);
  long filesize = lseek(fp, 0, SEEK_END);
  txt = (char *)mmap(NULL, filesize, PROT_READ, MAP_PRIVATE, fp, 0);

  // FILE *fP;
  // fP = fopen(testFile.c_str(), "r");
  // fseek(fP, 0, SEEK_END);
  // unsigned long filesize = ftell(fP);
  // rewind(fP);
  // char* txt = (char *)malloc(filesize * sizeof(char));
  // fread(txt,1,filesize,fP);

  test_num = (filesize + 100) / 6000;

  int subtxt_num = filesize / 8;
  int st0 = 0;
  int st1 = subtxt_num;
  int st2 = 2 * subtxt_num;
  int st3 = 3 * subtxt_num;

  int st4 = 4 * subtxt_num;
  int st5 = 5 * subtxt_num;
  int st6 = 6 * subtxt_num;
  int st7 = 7 * subtxt_num;

  thread pth0(subTestpredict, txt, predictVec, st0, st1 - 1, 0, param);
  thread pth1(subTestpredict, txt, predictVec + st1 / 6000, st1, st2 - 1, 1,
              param);
  thread pth2(subTestpredict, txt, predictVec + st2 / 6000, st2, st3 - 1, 2,
              param);
  thread pth3(subTestpredict, txt, predictVec + st3 / 6000, st3, st4 - 1, 3,
              param);

  thread pth4(subTestpredict, txt, predictVec + st4 / 6000, st4, st5 - 1, 4,
              param);
  thread pth5(subTestpredict, txt, predictVec + st5 / 6000, st5, st6 - 1, 5,
              param);
  thread pth6(subTestpredict, txt, predictVec + st6 / 6000, st6, st7 - 1, 6,
              param);
  thread pth7(subTestpredict, txt, predictVec + st7 / 6000, st7, filesize, 7,
              param);

  pth0.join();
  pth1.join();
  pth2.join();
  pth3.join();
  pth4.join();
  pth5.join();
  pth6.join();
  pth7.join();

  storePredict(predictVec);
}

int LR::storePredict(int *predictVec) {
  FILE *pFile;
  pFile = fopen(predictOutFile.c_str(), "w");

  for (int i = 0; i < test_num; i++) {
    fputc(*(predictVec + i) + '0', pFile);
    fputs("\n", pFile);
    // fprintf(pFile,"%d\n",*(predictVec+i));
  }

  fclose(pFile);
  return 0;
}

int main(int argc, char *argv[]) {
  string trainFile = "/data/train_data.txt";
  string testFile = "/data/test_data.txt";
  string predictFile = "/projects/student/result.txt";

  LR logist(trainFile, testFile, predictFile);

  logist.train();
  logist.predict();

  return 0;
}